{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/prashant/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/prashant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/prashant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from string import digits\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "stop=[]\n",
    "stop.extend(stopwords.words('english'))\n",
    "from textblob import TextBlob\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/prashant/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "word_set = set(word_list)\n",
    "\n",
    "def spelling_mistakes_check(sentences):\n",
    "    l = []\n",
    "    for i in sentences:\n",
    "        \n",
    "        cnt = 0\n",
    "        words = i.split(' ')\n",
    "        for w in words:\n",
    "            if w not in word_set:\n",
    "                cnt +=1\n",
    "        l.append(cnt)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def get_count_of_puncs(essay_list):\n",
    "    puncs = list(punctuation)\n",
    "    punc_count_list = []\n",
    "    for essay in essay_list:\n",
    "        c = 0\n",
    "        for punc in puncs:\n",
    "            if punc in essay:\n",
    "                c = c +1\n",
    "        punc_count_list.append(c)\n",
    "    return punc_count_list\n",
    "\n",
    "def get_number_of_char(essay_list):\n",
    "    num_char = []\n",
    "    for essay in essay_list:\n",
    "        num_char.append(len(essay))\n",
    "    return num_char\n",
    "\n",
    "def word_to_sent_ratio(essay_list):\n",
    "    w_s = []\n",
    "    for essay in essay_list:\n",
    "        num_sent = len(essay.split('.'))\n",
    "        num_word = len(essay.split(' '))\n",
    "        if num_word == 0:\n",
    "            w_s.append(0)\n",
    "        else:\n",
    "            w_s.append(num_sent/(num_word*1.0))\n",
    "    return w_s\n",
    "\n",
    "def get_avg_word_length_per_essay(essay_list):\n",
    "    avg = []\n",
    "    for essay in essay_list:\n",
    "        c = []\n",
    "        for w in essay.split():\n",
    "            c.append(len(w))\n",
    "        \n",
    "        if len(c) == 0:\n",
    "            avg.append(0)\n",
    "        else:\n",
    "            avg.append(sum(c)/len(c))\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "def sentence_to_wordlist(raw_sentence):\n",
    "    \n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# tokenizing an essay into a list of word lists\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def count_lemmas(essay):\n",
    "    \n",
    "    tokenized_sentences = tokenize(essay)      \n",
    "    \n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence) \n",
    "        \n",
    "        for token_tuple in tagged_tokens:\n",
    "        \n",
    "            pos_tag = token_tuple[1]\n",
    "        \n",
    "            if pos_tag.startswith('N'): \n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return lemma_count\n",
    "\n",
    "def count_lemma_for_all(essay_list):\n",
    "    lemma_count_list=[]\n",
    "    for essay in essay_list:\n",
    "        lemma_count_list.append(count_lemmas(essay))\n",
    "    return lemma_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_tags(sentences):\n",
    "    all_sample_tags = []\n",
    "    for i in sentences:\n",
    "        dict_count = {}\n",
    "        doc = nlp(i.decode('utf8'))\n",
    "        result = ([(X.label_) for X in doc.ents])\n",
    "\n",
    "        for tag in result:\n",
    "            if tag not in dict_count:\n",
    "                dict_count[tag] = 1\n",
    "            else:\n",
    "                dict_count[tag]  += 1\n",
    "        all_sample_tags.append(dict_count)\n",
    "    \n",
    "    cc = pd.DataFrame.from_dict(all_sample_tags)\n",
    "    cc.fillna(0,inplace=True)\n",
    "    return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(text):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "    vectorizer = CountVectorizer(min_df=20)\n",
    "    vectorizer.fit(text)\n",
    "    # encode document\n",
    "    vector = vectorizer.transform(text)\n",
    "    p = pd.DataFrame(vector.toarray())\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 02:09:14.617511 140315913066240 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 02:09:14.919055 140315913066240 monitored_session.py:222] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 02:09:15.743639 140315913066240 session_manager.py:491] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 02:09:16.883847 140315913066240 session_manager.py:493] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "#Function so that one session can be called multiple times. \n",
    "#Useful while multiple calls need to be done for embedding. \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "\n",
    "embed_fn = embed_useT('/home/prashant/google_sen_vec/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/miniconda2/envs/p3/lib/python3.6/site-packages/ipykernel_launcher.py:86: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_part_of_speech_tags(sentences):\n",
    "    all_sample_tags = []\n",
    "    for i in sentences:\n",
    "        dict_count = {}\n",
    "        result = (TextBlob(i))\n",
    "        for words, tag in result.tags:\n",
    "            if tag not in dict_count:\n",
    "                dict_count[tag] = 1\n",
    "            else:\n",
    "                dict_count[tag]  += 1\n",
    "        all_sample_tags.append(dict_count)\n",
    "    \n",
    "    cc = pd.DataFrame.from_dict(all_sample_tags)\n",
    "    cc.fillna(0,inplace=True)\n",
    "    return cc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_sen_len(sentences):\n",
    "    sen_len = []\n",
    "    for i in sentences:\n",
    "        i = len(i.split(' '))\n",
    "        sen_len.append(i)\n",
    "    return sen_len\n",
    "\n",
    "\n",
    "def get_noun_phrase(sentences):\n",
    "    l = []\n",
    "    for i in sentences:\n",
    "        doc = nlp(i)\n",
    "        cnt = 0\n",
    "        for chunk in doc.noun_chunks:\n",
    "            cnt +=1\n",
    "        l.append(cnt)\n",
    "    return l \n",
    "\n",
    "def pca(X):\n",
    "    import numpy as np\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=100)\n",
    "    pc = pca.fit_transform(X)\n",
    "    return pc\n",
    "\n",
    "def preprocess_data(s):\n",
    "    s = s.lower()\n",
    "    data = re.sub(r'[^\\x00-\\x7F]+', ' ', s)\n",
    "    final_str = data.translate(str.maketrans('', '', string.punctuation))\n",
    "    filter_str = final_str#.translate(str.maketrans('', '', digits))\n",
    "    nltk_tokens = nltk.word_tokenize(filter_str)\n",
    "    #Next find the roots of the word\n",
    "    str_= ''\n",
    "    for w in nltk_tokens:\n",
    "\n",
    "        if w not in stop:\n",
    "            str_ += ' '  + (w)\n",
    "    \n",
    "    return str_.strip()\n",
    "\n",
    "\n",
    "def get_vector(model,all_data):\n",
    "    vector_sen = []\n",
    "    for d in all_data:\n",
    "        single_sen_vec = []\n",
    "        words = d.split(' ')\n",
    "        for w in words:\n",
    "            try:   \n",
    "                get_word_vec = model[w]\n",
    "            except:\n",
    "                pass\n",
    "            single_sen_vec.append(get_word_vec)\n",
    "        v = np.array(single_sen_vec).mean(axis=0)\n",
    "        vector_sen.append(v)\n",
    "    return vector_sen\n",
    "\n",
    "\n",
    "#vocab = model.keys()\n",
    "\n",
    "df_test = pd.read_csv('test_dataset.csv')\n",
    "df_test.head()\n",
    "df_train = pd.read_csv('train_dataset.csv')\n",
    "\n",
    "df_train['source_'] = 1\n",
    "df_test['source_'] =0\n",
    "\n",
    "df = pd.concat([df_train,df_test])\n",
    "df['score_1'].fillna(1, inplace = True)\n",
    "df['score_2'].fillna(1, inplace = True)\n",
    "df['score_3'].fillna(1, inplace = True)\n",
    "df['score_4'].fillna(1, inplace = True)\n",
    "df['score_5'].fillna(1, inplace = True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'],inplace=True,axis=1)\n",
    "\n",
    "multi_datasets = {}\n",
    "groups = df.groupby('Essayset')\n",
    "for name, group in groups:\n",
    "    multi_datasets[name] = group\n",
    "\n",
    "pp = []\n",
    "def different_models(model,multi_datasets):\n",
    "    flag = 1\n",
    "    for i in multi_datasets:\n",
    "        print(i)\n",
    "        \n",
    "        single_df = multi_datasets[i]\n",
    "        single_df.reset_index(inplace= True)\n",
    "        single_df.dropna(axis=0,inplace=True)\n",
    "\n",
    "        test_id = list(single_df.loc[single_df['source_'] ==0]['ID'])\n",
    "        es = list(single_df.loc[single_df['source_'] ==0]['Essayset'])\n",
    "        if len(test_id) != len(es):\n",
    "            print('PANGA')\n",
    "            break\n",
    "\n",
    "        single_df['candi_score'] = single_df[['score_1','score_2' ,'score_3' ,'score_4' ,'score_5']].mean(axis=1)\n",
    "        single_df['candi_score'] = list(map(lambda x : round(x),single_df['candi_score']))\n",
    "        single_df.drop(['score_1','score_2' ,'score_3' ,'score_4' ,'score_5','ID','index'],inplace=True,axis=1)\n",
    "\n",
    "        single_df['kitna_aacha']=single_df['candi_score']\n",
    "\n",
    "        single_df.drop(['min_score','max_score','Essayset','candi_score'],inplace=True,axis=1)\n",
    "\n",
    "        #all_text  = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        parts_dataset = get_part_of_speech_tags(list(single_df['EssayText']))\n",
    "        #ner_tags = get_ner_tags(list(single_df['EssayText']))\n",
    "        #noun_p = get_noun_phrase(list(single_df['EssayText']))\n",
    "        #vectors = pd.DataFrame(embed_fn(list(single_df['EssayText'])))\n",
    "        punc_count = get_count_of_puncs(list(single_df['EssayText']))\n",
    "        lemma_cnt =count_lemma_for_all(list(single_df['EssayText']))\n",
    "        \n",
    "        single_df['EssayText']=single_df['EssayText'].apply(preprocess_data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        mistakes = spelling_mistakes_check(list(single_df['EssayText']))\n",
    "        sen_len = get_sen_len(list(single_df['EssayText']))\n",
    "        \n",
    "        num_char = get_number_of_char(list(single_df['EssayText']))\n",
    "\n",
    "\n",
    "        word_sent_ratio = word_to_sent_ratio(list(single_df['EssayText']))\n",
    "\n",
    "        avg_len = get_avg_word_length_per_essay(list(single_df['EssayText']))\n",
    "\n",
    "        \n",
    "        #tfidf_vec = tfidf(list(single_df['EssayText']))\n",
    "        print('vectors_fetched!!')\n",
    "        #all_text = list(single_df['EssayText'])\n",
    "        #vectors = get_vector(model,list(single_df['EssayText']))\n",
    "\n",
    "        #from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        #tfidf = TfidfVectorizer(sublinear_tf=True,use_idf=True, min_df=20, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "        #tfidf.fit(all_text)\n",
    "\n",
    "        #features = tfidf.transform(single_df['EssayText'])\n",
    "\n",
    "        new_tfidf_features = pd.DataFrame()\n",
    "        new_tfidf_features['clarity'] = list(single_df['clarity'])\n",
    "        new_tfidf_features['sen_len'] = sen_len\n",
    "        #new_tfidf_features['noun_p'] = noun_p\n",
    "        new_tfidf_features['coherent'] = list(single_df['coherent'])\n",
    "        new_tfidf_features['mistakes'] = mistakes\n",
    "        #new_tfidf_features['lc'] = lemma_cnt\n",
    "        \n",
    "        new_tfidf_features['punc_count'] = punc_count\n",
    "        new_tfidf_features['num_char'] = num_char\n",
    "        new_tfidf_features['word_sent_r'] = word_sent_ratio\n",
    "        new_tfidf_features['avg_len'] = avg_len\n",
    "        \n",
    "        \n",
    "        \n",
    "        new_tfidf_features['kitna_aacha'] = list(single_df['kitna_aacha'])\n",
    "        new_tfidf_features['source_'] = list(single_df['source_'])\n",
    "        \n",
    "        \n",
    "        new_tfidf_features = pd.concat([new_tfidf_features,parts_dataset],axis=1)\n",
    "        print(new_tfidf_features.shape)\n",
    "        \n",
    "        new_tfidf_features.dropna(axis=0,inplace=True)\n",
    "        all_preprocessed_single_q_data =  pd.get_dummies(new_tfidf_features,columns=['clarity','coherent'])\n",
    "        preprocessed_main_test = all_preprocessed_single_q_data.loc[all_preprocessed_single_q_data['source_'] == 0]\n",
    "        preprocessed_main_train = all_preprocessed_single_q_data.loc[all_preprocessed_single_q_data['source_'] == 1]\n",
    "\n",
    "        preprocessed_main_test.drop(['source_','kitna_aacha'],axis =1,inplace=True)\n",
    "\n",
    "        y_train = (preprocessed_main_train['kitna_aacha'])\n",
    "        x_train = preprocessed_main_train.drop(['kitna_aacha','source_'],axis =1)\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.naive_bayes import MultinomialNB,GaussianNB\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        X_train, X_test, y_train_internal, y_test_internal = train_test_split(x_train,y_train ,test_size =0)\n",
    "    \n",
    "        num_cla = len(set(y_train))\n",
    "        print('----')\n",
    "        print(num_cla)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        params = {\n",
    "'num_leaves': 60,\n",
    "'max_bin': 110,\n",
    "'min_data_in_leaf': 50,\n",
    "'learning_rate': 0.01,\n",
    "'min_sum_hessian_in_leaf': 0.000446,\n",
    "'bagging_fraction': 0.60,\n",
    "'bagging_freq': 15,\n",
    "'max_depth': 20,\n",
    "'save_binary': True,\n",
    "'seed': 31452,\n",
    "'feature_fraction_seed': 31415,\n",
    "'feature_fraction': 0.51,\n",
    "'bagging_seed': 31415,\n",
    "'drop_seed': 31415,\n",
    "'data_random_seed': 31415,\n",
    "'objective': 'regression',\n",
    "'boosting_type': 'gbdt',\n",
    "'verbose': 1,\n",
    "'metric': 'rmse',\n",
    "'is_unbalance': False,\n",
    "#     'categorical_feature': \"name:branch_id\"\n",
    "# 'scale_pos_weight': len(train_df[\"target\"] == 0) / sum(train_df[\"target\"])\n",
    "}\n",
    "\n",
    "# Import the model we are using\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "# from tpot import TPOTClassifier\n",
    "        import lightgbm as lgb\n",
    "        print(X_train.shape)\n",
    "        print(y_train_internal.shape)\n",
    "        d_train = lgb.Dataset(X_train, label=y_train_internal)\n",
    "# Instantiate model with 1000 decision trees\n",
    "#rf = RandomForestClassifier()\n",
    "\n",
    "# Train the model on training data\n",
    "#rm = rf.fit(X_train, y_train_internal)\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "        #n_estimators = clf.best_iteration\n",
    "\n",
    "\n",
    "\n",
    "        clf = lgb.train(params, d_train,30000)\n",
    "\n",
    "        #pred_internal = rm.predict(preprocessed_main_test)\n",
    "        #res =list(zip(test_id,es,pred_internal))\n",
    "        #from sklearn import metrics\n",
    "        if flag ==0 :\n",
    "            pred_internal = clf.predict(X_test)\n",
    "            print(pred_internal)\n",
    "            pred_internal = list(map(lambda x : max(enumerate(x), key=operator.itemgetter(1))[0],pred_internal))\n",
    "            from sklearn import metrics\n",
    "            pp.append(metrics.accuracy_score(y_test_internal, pred_internal))\n",
    "        if flag ==1:\n",
    "            pred_internal = clf.predict(preprocessed_main_test)\n",
    "            #pred_internal = list(map(lambda x : max(enumerate(x), key=operator.itemgetter(1))[0],pred_internal))\n",
    "            res =list(zip(test_id,es,pred_internal))\n",
    "            pp.append(res)\n",
    "    return pp\n",
    "    #res =list(zip(test_id,pred_internal))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.keyedvectors import KeyedVectors\n",
    "# model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "vectors_fetched!!\n",
      "(2195, 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/miniconda2/envs/p3/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "4\n",
      "(1638, 49)\n",
      "(1638,)\n",
      "2.0\n",
      "vectors_fetched!!\n",
      "(1679, 43)\n",
      "----\n",
      "4\n",
      "(1253, 47)\n",
      "(1253,)\n",
      "3.0\n",
      "vectors_fetched!!\n",
      "(2164, 45)\n",
      "----\n",
      "3\n",
      "(1758, 49)\n",
      "(1758,)\n",
      "4.0\n",
      "vectors_fetched!!\n",
      "(1905, 44)\n",
      "----\n",
      "3\n",
      "(1610, 48)\n",
      "(1610,)\n",
      "5.0\n",
      "vectors_fetched!!\n",
      "(2343, 43)\n",
      "----\n",
      "4\n",
      "(1745, 47)\n",
      "(1745,)\n",
      "6.0\n",
      "vectors_fetched!!\n",
      "(2359, 45)\n",
      "----\n",
      "4\n",
      "(1760, 49)\n",
      "(1760,)\n",
      "7.0\n",
      "vectors_fetched!!\n",
      "(2347, 43)\n",
      "----\n",
      "3\n",
      "(1748, 47)\n",
      "(1748,)\n",
      "8.0\n",
      "vectors_fetched!!\n",
      "(2350, 44)\n",
      "----\n",
      "3\n",
      "(1751, 48)\n",
      "(1751,)\n",
      "9.0\n",
      "vectors_fetched!!\n",
      "(2352, 44)\n",
      "----\n",
      "3\n",
      "(1753, 48)\n",
      "(1753,)\n",
      "10.0\n",
      "vectors_fetched!!\n",
      "(2133, 44)\n",
      "----\n",
      "3\n",
      "(1587, 48)\n",
      "(1587,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "a = different_models('dcd',multi_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import itertools\n",
    "l = list(itertools.chain(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1673, 1.0, 0.887353247670009),\n",
       " (1674, 1.0, 0.6228040486059262),\n",
       " (1675, 1.0, 2.683417578841114),\n",
       " (1676, 1.0, 0.21038973747620227),\n",
       " (1677, 1.0, -0.07435227367987401),\n",
       " (1678, 1.0, 2.278907765699428),\n",
       " (1679, 1.0, 0.1896145430054261),\n",
       " (1680, 1.0, 0.8023723919200301),\n",
       " (1681, 1.0, 1.9776557823109384),\n",
       " (1682, 1.0, 2.2520263441392427),\n",
       " (1683, 1.0, 1.9655396136569998),\n",
       " (1684, 1.0, 0.9487287724274194),\n",
       " (1685, 1.0, 0.32171181017614436),\n",
       " (1686, 1.0, 1.8515066830937976),\n",
       " (1687, 1.0, 1.0141790666707335),\n",
       " (1688, 1.0, 2.2916666160796395),\n",
       " (1689, 1.0, 2.3732825643981608),\n",
       " (1690, 1.0, 0.0842344854653409),\n",
       " (1691, 1.0, 1.0590855573091265),\n",
       " (1692, 1.0, 2.377857634034379),\n",
       " (1693, 1.0, 0.5280818248985901),\n",
       " (1694, 1.0, 0.13434201548017632),\n",
       " (1695, 1.0, 0.30284318315831593),\n",
       " (1696, 1.0, 0.7657721928011895),\n",
       " (1697, 1.0, 1.0376206767043838),\n",
       " (1698, 1.0, 0.22555771011316744),\n",
       " (1699, 1.0, 2.90000650602914),\n",
       " (1700, 1.0, 2.5560939731637777),\n",
       " (1701, 1.0, 2.5165110025015207),\n",
       " (1702, 1.0, 2.2537270370246048),\n",
       " (1703, 1.0, 2.8365483019536213),\n",
       " (1704, 1.0, 1.966049516212928),\n",
       " (1705, 1.0, 2.400572228296104),\n",
       " (1706, 1.0, 2.323803244585845),\n",
       " (1707, 1.0, 0.9054158990632171),\n",
       " (1708, 1.0, 2.1642727637558505),\n",
       " (1709, 1.0, 2.3390980189746506),\n",
       " (1710, 1.0, 2.554481861965921),\n",
       " (1711, 1.0, 2.410580540779254),\n",
       " (1712, 1.0, 2.455336370527701),\n",
       " (1713, 1.0, 0.44056595306321905),\n",
       " (1714, 1.0, 1.0261368847194343),\n",
       " (1715, 1.0, 2.2233945215386903),\n",
       " (1716, 1.0, -0.2916713575684666),\n",
       " (1717, 1.0, 1.8959665128300538),\n",
       " (1718, 1.0, 0.4256617225007999),\n",
       " (1719, 1.0, -0.15978369902208076),\n",
       " (1720, 1.0, 2.2989823416952215),\n",
       " (1721, 1.0, 2.3697632899209906),\n",
       " (1722, 1.0, 1.1842367390304198),\n",
       " (1723, 1.0, 2.55967315910558),\n",
       " (1724, 1.0, 0.6324405968986115),\n",
       " (1725, 1.0, 0.7931005669439716),\n",
       " (1726, 1.0, 0.5044355518999724),\n",
       " (1727, 1.0, 2.5296029473574886),\n",
       " (1728, 1.0, 0.8013047436991567),\n",
       " (1729, 1.0, 0.527757888352593),\n",
       " (1730, 1.0, 0.23729431925251704),\n",
       " (1731, 1.0, 2.4680668839941395),\n",
       " (1732, 1.0, 2.2478292292661344),\n",
       " (1733, 1.0, 0.2399448790984136),\n",
       " (1734, 1.0, 2.5659885178129467),\n",
       " (1735, 1.0, 0.8892877107229946),\n",
       " (1736, 1.0, 2.637272133279658),\n",
       " (1737, 1.0, 2.478239831702642),\n",
       " (1738, 1.0, 0.1959785437898657),\n",
       " (1739, 1.0, -0.024412983721036768),\n",
       " (1740, 1.0, 2.439014415022079),\n",
       " (1741, 1.0, 0.8890801018895717),\n",
       " (1742, 1.0, 0.17814567326786104),\n",
       " (1743, 1.0, 1.933955390563383),\n",
       " (1744, 1.0, 0.48035228017733145),\n",
       " (1745, 1.0, -0.058554827461702416),\n",
       " (1746, 1.0, 0.42034451318035604),\n",
       " (1747, 1.0, 2.4274215183075647),\n",
       " (1748, 1.0, 2.370356835369301),\n",
       " (1749, 1.0, 2.267049904841478),\n",
       " (1750, 1.0, 1.0270755745793838),\n",
       " (1751, 1.0, 1.041719667313208),\n",
       " (1752, 1.0, 2.1014861399850795),\n",
       " (1753, 1.0, 0.08583204909286242),\n",
       " (1754, 1.0, 2.3618978564735786),\n",
       " (1755, 1.0, 2.718764746824639),\n",
       " (1756, 1.0, 2.4290471358502743),\n",
       " (1757, 1.0, 2.5175755840383185),\n",
       " (1758, 1.0, 2.2423422564351907),\n",
       " (1759, 1.0, 2.252367112526103),\n",
       " (1760, 1.0, 1.862316578088488),\n",
       " (1761, 1.0, 2.7785131640856315),\n",
       " (1762, 1.0, 0.2100601231903204),\n",
       " (1763, 1.0, 2.2572220671126835),\n",
       " (1764, 1.0, 0.5344106369246577),\n",
       " (1765, 1.0, 1.5431326163058143),\n",
       " (1766, 1.0, 0.7114249611622604),\n",
       " (1767, 1.0, 2.4274503998516033),\n",
       " (1768, 1.0, 0.144746967024091),\n",
       " (1769, 1.0, 2.1666372042995836),\n",
       " (1770, 1.0, 2.361511652204229),\n",
       " (1771, 1.0, 2.655810855488716),\n",
       " (1772, 1.0, 2.6768192249742406),\n",
       " (1773, 1.0, 2.5536088030372888),\n",
       " (1774, 1.0, 0.4786026168808806),\n",
       " (1775, 1.0, 2.350009566786797),\n",
       " (1776, 1.0, 2.0947862788353198),\n",
       " (1777, 1.0, 0.9107710873389527),\n",
       " (1778, 1.0, 2.7035154260384218),\n",
       " (1779, 1.0, 0.17874040595523788),\n",
       " (1780, 1.0, 0.6076483758203435),\n",
       " (1781, 1.0, 2.606388109343419),\n",
       " (1782, 1.0, -0.46174680738591056),\n",
       " (1783, 1.0, 2.5509235363136464),\n",
       " (1784, 1.0, 2.1888776660018805),\n",
       " (1785, 1.0, 0.959342833015744),\n",
       " (1786, 1.0, 0.22651778443652457),\n",
       " (1787, 1.0, 2.3920075969813177),\n",
       " (1788, 1.0, 0.5376995063059287),\n",
       " (1789, 1.0, 2.4393184464549553),\n",
       " (1790, 1.0, 0.25261158701291136),\n",
       " (1791, 1.0, 2.1569773009718607),\n",
       " (1792, 1.0, 2.4432808707268943),\n",
       " (1793, 1.0, 0.3613593290553118),\n",
       " (1794, 1.0, -0.12185330447914827),\n",
       " (1795, 1.0, 2.0070227825213096),\n",
       " (1796, 1.0, 0.4673154417570021),\n",
       " (1797, 1.0, -0.21201185615033985),\n",
       " (1798, 1.0, 2.376466821389543),\n",
       " (1799, 1.0, 0.40809113934543123),\n",
       " (1800, 1.0, -0.15367377719852537),\n",
       " (1801, 1.0, 2.2519802022562967),\n",
       " (1802, 1.0, 0.8721073946093933),\n",
       " (1803, 1.0, 2.1523848421828076),\n",
       " (1804, 1.0, 2.587370441897495),\n",
       " (1805, 1.0, 0.49845632545154156),\n",
       " (1806, 1.0, 0.5054795235892129),\n",
       " (1807, 1.0, 2.343183508843238),\n",
       " (1808, 1.0, 2.3824389634512557),\n",
       " (1809, 1.0, 0.6299810557547396),\n",
       " (1810, 1.0, 0.45498388524015937),\n",
       " (1811, 1.0, 0.9264416751094766),\n",
       " (1812, 1.0, 0.6002687809061579),\n",
       " (1813, 1.0, 2.4899876862394548),\n",
       " (1814, 1.0, 2.9217651908906217),\n",
       " (1815, 1.0, 0.19728577186412477),\n",
       " (1816, 1.0, 0.11345945244864289),\n",
       " (1817, 1.0, 2.2532973854031004),\n",
       " (1818, 1.0, 2.260532722693486),\n",
       " (1819, 1.0, 2.743335195750652),\n",
       " (1820, 1.0, 0.32536555725914007),\n",
       " (1821, 1.0, 0.5296916897140246),\n",
       " (1822, 1.0, 2.9335338380576346),\n",
       " (1823, 1.0, 2.3554732112614185),\n",
       " (1824, 1.0, 0.9431233159039174),\n",
       " (1825, 1.0, 0.6444994830896479),\n",
       " (1826, 1.0, 0.18388475969900814),\n",
       " (1827, 1.0, 2.0234918149316594),\n",
       " (1828, 1.0, 2.194928843648621),\n",
       " (1829, 1.0, 2.729996004733346),\n",
       " (1830, 1.0, 2.1619274265971837),\n",
       " (1831, 1.0, 2.3806641978970617),\n",
       " (1832, 1.0, 2.449404285880243),\n",
       " (1833, 1.0, 0.38362226265921007),\n",
       " (1834, 1.0, 2.61853455384689),\n",
       " (1835, 1.0, 1.2038801893801527),\n",
       " (1836, 1.0, 2.712288519815437),\n",
       " (1837, 1.0, 0.8497154701398374),\n",
       " (1838, 1.0, 0.7705133172990434),\n",
       " (1839, 1.0, 2.6188042609784574),\n",
       " (1840, 1.0, 0.9318333295175404),\n",
       " (1841, 1.0, -0.10079705845100939),\n",
       " (1842, 1.0, 1.9594084467717126),\n",
       " (1843, 1.0, 3.096526009934932),\n",
       " (1844, 1.0, 2.468926200948552),\n",
       " (1845, 1.0, 0.27730372913896695),\n",
       " (1846, 1.0, 2.4410585115143415),\n",
       " (1847, 1.0, 2.390493271530439),\n",
       " (1848, 1.0, 1.0938345108988392),\n",
       " (1849, 1.0, -0.1893140864224242),\n",
       " (1850, 1.0, 2.499131580944648),\n",
       " (1851, 1.0, 0.43456303295625864),\n",
       " (1852, 1.0, 0.04326970534174093),\n",
       " (1853, 1.0, 0.48773858381681157),\n",
       " (1854, 1.0, 2.5324385712329027),\n",
       " (1855, 1.0, 2.2864624942109186),\n",
       " (1856, 1.0, 0.39815271489331744),\n",
       " (1857, 1.0, 2.084722007041762),\n",
       " (1858, 1.0, 1.7674979081980775),\n",
       " (1859, 1.0, 2.238010085773952),\n",
       " (1860, 1.0, 1.9407178796911764),\n",
       " (1861, 1.0, 0.8199008277755024),\n",
       " (1862, 1.0, 2.2903488521399433),\n",
       " (1863, 1.0, -0.06113214872636301),\n",
       " (1864, 1.0, 2.1258759995451606),\n",
       " (1865, 1.0, 0.8739741983163484),\n",
       " (1866, 1.0, 0.6234102523010313),\n",
       " (1867, 1.0, 1.170023431658665),\n",
       " (1868, 1.0, 0.8794503475456729),\n",
       " (1869, 1.0, 0.6578290493762049),\n",
       " (1870, 1.0, 2.536644157515432),\n",
       " (1871, 1.0, 0.17047270564558592),\n",
       " (1872, 1.0, 2.298596697308644),\n",
       " (1873, 1.0, -0.209707118412486),\n",
       " (1874, 1.0, 2.7809172615399285),\n",
       " (1875, 1.0, 2.616693989916553),\n",
       " (1876, 1.0, 2.2605785144760415),\n",
       " (1877, 1.0, 2.256849569287521),\n",
       " (1878, 1.0, 0.4261117825494366),\n",
       " (1879, 1.0, 2.239404363572705),\n",
       " (1880, 1.0, 2.495537404632137),\n",
       " (1881, 1.0, 0.6867343026998479),\n",
       " (1882, 1.0, 2.646528554870983),\n",
       " (1883, 1.0, 1.9880663888588885),\n",
       " (1884, 1.0, 0.652989475810454),\n",
       " (1885, 1.0, 2.157788740855276),\n",
       " (1886, 1.0, 2.4495088093226407),\n",
       " (1887, 1.0, 1.8678204547510906),\n",
       " (1888, 1.0, 2.5585807784542),\n",
       " (1889, 1.0, 0.32593036640283063),\n",
       " (1890, 1.0, 2.4641100847434685),\n",
       " (1891, 1.0, 0.5291558915608957),\n",
       " (1892, 1.0, 2.4318138104130735),\n",
       " (1893, 1.0, 0.3135267771366647),\n",
       " (1894, 1.0, 2.164473368475009),\n",
       " (1895, 1.0, 2.139875724781185),\n",
       " (1896, 1.0, 0.4148138859039173),\n",
       " (1897, 1.0, 2.1439236477074557),\n",
       " (1898, 1.0, 0.6140959464258595),\n",
       " (1899, 1.0, 0.5990506355612729),\n",
       " (1900, 1.0, 1.2052555401154776),\n",
       " (1901, 1.0, 0.7987618060400512),\n",
       " (1902, 1.0, 0.6032102694049707),\n",
       " (1903, 1.0, 0.7927029190668539),\n",
       " (1904, 1.0, 1.2462119576373845),\n",
       " (1905, 1.0, 0.642316339402673),\n",
       " (1906, 1.0, 2.503649794724363),\n",
       " (1907, 1.0, 2.1280828603336452),\n",
       " (1908, 1.0, 2.3639503448820345),\n",
       " (1909, 1.0, 2.6076185766867725),\n",
       " (1910, 1.0, 1.6560292603314388),\n",
       " (1911, 1.0, 2.1471497676218596),\n",
       " (1912, 1.0, 0.6551158459426856),\n",
       " (1913, 1.0, 0.2157148135425337),\n",
       " (1914, 1.0, 0.44497182777897654),\n",
       " (1915, 1.0, 2.5776027083488477),\n",
       " (1916, 1.0, 0.35030294230952336),\n",
       " (1917, 1.0, 0.2459787346425853),\n",
       " (1918, 1.0, 0.40381239919934253),\n",
       " (1919, 1.0, 0.5992632333302171),\n",
       " (1920, 1.0, 1.2157746066998085),\n",
       " (1921, 1.0, 0.7358489112029035),\n",
       " (1922, 1.0, 0.7884380980654623),\n",
       " (1923, 1.0, 0.7912442523878166),\n",
       " (1924, 1.0, -0.16645733463608817),\n",
       " (1925, 1.0, 2.4642219366389617),\n",
       " (1926, 1.0, 2.3786128168969425),\n",
       " (1927, 1.0, 2.4162276773390916),\n",
       " (1928, 1.0, 2.295472134168133),\n",
       " (1929, 1.0, 2.354961842773825),\n",
       " (1930, 1.0, -0.02459408164544178),\n",
       " (1931, 1.0, 0.12286404627573791),\n",
       " (1932, 1.0, 2.4663986290089883),\n",
       " (1933, 1.0, 2.043619352073317),\n",
       " (1934, 1.0, 2.702326820622771),\n",
       " (1935, 1.0, 0.7633005084273902),\n",
       " (1936, 1.0, 3.0174285780968235),\n",
       " (1937, 1.0, 2.522006014727948),\n",
       " (1938, 1.0, 2.597259533261292),\n",
       " (1939, 1.0, 1.9265187588279147),\n",
       " (1940, 1.0, 0.507959205270885),\n",
       " (1941, 1.0, 2.292548723985627),\n",
       " (1942, 1.0, 2.965115469367509),\n",
       " (1943, 1.0, -0.27200490607563027),\n",
       " (1944, 1.0, 2.7520782390132514),\n",
       " (1945, 1.0, 1.2931253470598945),\n",
       " (1946, 1.0, 1.852351165565677),\n",
       " (1947, 1.0, 0.6008437172991695),\n",
       " (1948, 1.0, 0.4914245241839108),\n",
       " (1949, 1.0, 0.3710103758750132),\n",
       " (1950, 1.0, 0.11150895166888085),\n",
       " (1951, 1.0, 0.25865713197710294),\n",
       " (1952, 1.0, 0.909452395059002),\n",
       " (1953, 1.0, 0.33872325500635875),\n",
       " (1954, 1.0, 0.21019983320939548),\n",
       " (1955, 1.0, 0.32208656630978555),\n",
       " (1956, 1.0, 0.2751511611608685),\n",
       " (1957, 1.0, 0.7454418380232816),\n",
       " (1958, 1.0, 2.045092158445478),\n",
       " (1959, 1.0, 2.0900524957364737),\n",
       " (1960, 1.0, 2.0615508815237535),\n",
       " (1961, 1.0, 0.4581436152414854),\n",
       " (1962, 1.0, 2.3186254884368327),\n",
       " (1963, 1.0, 1.9389537281706906),\n",
       " (1964, 1.0, 2.4114940580028263),\n",
       " (1965, 1.0, 0.15948165663051125),\n",
       " (1966, 1.0, 2.5599420251417224),\n",
       " (1967, 1.0, 1.1280108361607637),\n",
       " (1968, 1.0, 2.0718175809478825),\n",
       " (1969, 1.0, 2.1253313488456),\n",
       " (1970, 1.0, 0.4728974743092649),\n",
       " (1971, 1.0, 0.23381826717975832),\n",
       " (1972, 1.0, 0.4500941701006863),\n",
       " (1973, 1.0, 2.355281554673296),\n",
       " (1974, 1.0, 1.1626246725269762),\n",
       " (1975, 1.0, 0.6193275662037898),\n",
       " (1976, 1.0, -0.4357511158794447),\n",
       " (1977, 1.0, 2.131686503391099),\n",
       " (1978, 1.0, 0.6474179726523066),\n",
       " (1979, 1.0, 2.0340896121425907),\n",
       " (1980, 1.0, 2.370116481767036),\n",
       " (1981, 1.0, 1.8572199180863738),\n",
       " (1982, 1.0, 2.4409136581751363),\n",
       " (1983, 1.0, 0.32554459940945274),\n",
       " (1984, 1.0, 2.3408224143610665),\n",
       " (1985, 1.0, 2.219706659415122),\n",
       " (1986, 1.0, 2.5707234875900764),\n",
       " (1987, 1.0, -0.071674590751146),\n",
       " (1988, 1.0, 0.9064838373353886),\n",
       " (1989, 1.0, 0.7446025843164817),\n",
       " (1990, 1.0, 2.0734637247490557),\n",
       " (1991, 1.0, 2.4354549475440828),\n",
       " (1992, 1.0, 0.3659481551168718),\n",
       " (1993, 1.0, 0.38735196227200797),\n",
       " (1994, 1.0, 2.8633969092370997),\n",
       " (1995, 1.0, 2.8383525030104404),\n",
       " (1996, 1.0, 0.7649115131557617),\n",
       " (1997, 1.0, 1.9482713365889848),\n",
       " (1998, 1.0, 2.2367361393272827),\n",
       " (1999, 1.0, 2.191837671130353),\n",
       " (2000, 1.0, 0.4290955962760273),\n",
       " (2001, 1.0, 2.7134655495174833),\n",
       " (2002, 1.0, 0.18798563003278),\n",
       " (2003, 1.0, 1.979831430200547),\n",
       " (2004, 1.0, 2.4732862990245286),\n",
       " (2005, 1.0, 1.885096197650996),\n",
       " (2006, 1.0, 2.860487079977069),\n",
       " (2007, 1.0, 0.03428356921973081),\n",
       " (2008, 1.0, 2.79251246348189),\n",
       " (2009, 1.0, 1.9906351228285037),\n",
       " (2010, 1.0, 2.257224685287852),\n",
       " (2011, 1.0, 0.5079776805672102),\n",
       " (2012, 1.0, 2.4128462105695685),\n",
       " (2013, 1.0, 2.109321663445978),\n",
       " (2014, 1.0, 2.826436150843205),\n",
       " (2015, 1.0, 0.8193264631809747),\n",
       " (2016, 1.0, 2.1119480065280754),\n",
       " (2017, 1.0, 2.370836197964407),\n",
       " (2018, 1.0, 0.6107905413174255),\n",
       " (2019, 1.0, 1.9908698877317832),\n",
       " (2020, 1.0, 0.47703347095577225),\n",
       " (2021, 1.0, 2.9513322501772223),\n",
       " (2022, 1.0, 2.352423345909744),\n",
       " (2023, 1.0, 0.8998100779071398),\n",
       " (2024, 1.0, 2.818737775744737),\n",
       " (2025, 1.0, 0.8530862600854403),\n",
       " (2026, 1.0, 1.9566467402575212),\n",
       " (2027, 1.0, 0.003747695582271865),\n",
       " (2028, 1.0, 2.2865435331834885),\n",
       " (2029, 1.0, 0.13706738886075875),\n",
       " (2030, 1.0, 0.5994854551377102),\n",
       " (2031, 1.0, 0.4500304794962854),\n",
       " (2032, 1.0, 0.4655012061324857),\n",
       " (2033, 1.0, 2.284291836975273),\n",
       " (2034, 1.0, 2.442805893294653),\n",
       " (2035, 1.0, 0.2862036234435166),\n",
       " (2036, 1.0, 0.7905126362545306),\n",
       " (2037, 1.0, 2.4930618682365666),\n",
       " (2038, 1.0, 1.9849285752612447),\n",
       " (2039, 1.0, 0.49484776659746954),\n",
       " (2040, 1.0, 2.3940017276289502),\n",
       " (2041, 1.0, 2.089299178826299),\n",
       " (2042, 1.0, 2.5022127907176803),\n",
       " (2043, 1.0, 2.3380862097556157),\n",
       " (2044, 1.0, 1.0100707475924078),\n",
       " (2045, 1.0, 2.3514986642018214),\n",
       " (2046, 1.0, 0.9946380483840437),\n",
       " (2047, 1.0, 0.5076273043552669),\n",
       " (2048, 1.0, 2.577977498199114),\n",
       " (2049, 1.0, 0.042850374016753574),\n",
       " (2050, 1.0, 0.6504743657317275),\n",
       " (2051, 1.0, 0.11814001503308474),\n",
       " (2052, 1.0, 2.320703552724686),\n",
       " (2053, 1.0, 2.931353864729049),\n",
       " (2054, 1.0, -0.1129727186502967),\n",
       " (2055, 1.0, 0.6496481534440423),\n",
       " (2056, 1.0, 2.3848032295900525),\n",
       " (2057, 1.0, 2.647326894297124),\n",
       " (2058, 1.0, 2.472560637840108),\n",
       " (2059, 1.0, 2.5389510523924965),\n",
       " (2060, 1.0, 2.6556037375424757),\n",
       " (2061, 1.0, 0.508843163957329),\n",
       " (2062, 1.0, 0.011682500535474535),\n",
       " (2063, 1.0, 1.2173357572633923),\n",
       " (2064, 1.0, 0.5318912778263288),\n",
       " (2065, 1.0, 0.5910684268319117),\n",
       " (2066, 1.0, 0.5820574215971522),\n",
       " (2067, 1.0, -0.1623215881070403),\n",
       " (2068, 1.0, -0.3616576851391119),\n",
       " (2069, 1.0, 0.16826980293342703),\n",
       " (2070, 1.0, 2.9264458893819647),\n",
       " (2071, 1.0, 0.11360686751483236),\n",
       " (2072, 1.0, 2.492137245927211),\n",
       " (2073, 1.0, 0.028225094583986924),\n",
       " (2074, 1.0, 2.5437807348758175),\n",
       " (2075, 1.0, 2.459778421068745),\n",
       " (2076, 1.0, 2.407012881283959),\n",
       " (2077, 1.0, 0.4320572818515015),\n",
       " (2078, 1.0, 2.5914628348082256),\n",
       " (2079, 1.0, 2.284914136471424),\n",
       " (2080, 1.0, -0.02995692750508651),\n",
       " (2081, 1.0, 2.3482440568747083),\n",
       " (2082, 1.0, -0.2231237344318658),\n",
       " (2083, 1.0, 2.7234780158360445),\n",
       " (2084, 1.0, 2.870946956441675),\n",
       " (2085, 1.0, 0.32162088292433944),\n",
       " (2086, 1.0, 1.1170708189837746),\n",
       " (2087, 1.0, 2.1386859524997086),\n",
       " (2088, 1.0, 2.200763680015215),\n",
       " (2089, 1.0, 0.8970732645263769),\n",
       " (2090, 1.0, 1.730605995328197),\n",
       " (2091, 1.0, 2.0970423632264525),\n",
       " (2092, 1.0, 2.948779197210648),\n",
       " (2093, 1.0, 0.6732409799259194),\n",
       " (2094, 1.0, 2.3229531806611012),\n",
       " (2095, 1.0, 0.22715708666077972),\n",
       " (2096, 1.0, 0.3499126595744241),\n",
       " (2097, 1.0, 2.291875877528282),\n",
       " (2098, 1.0, 2.2075602591025523),\n",
       " (2099, 1.0, 2.23807692477602),\n",
       " (2100, 1.0, 2.6411015860104246),\n",
       " (2101, 1.0, 0.13820008619075855),\n",
       " (2102, 1.0, 2.521759890544726),\n",
       " (2103, 1.0, 2.281735928560291),\n",
       " (2104, 1.0, 0.791770037613434),\n",
       " (2105, 1.0, 0.18933361526997528),\n",
       " (2106, 1.0, 0.40196708979582674),\n",
       " (2107, 1.0, 2.273642659637455),\n",
       " (2108, 1.0, 2.5166885835661326),\n",
       " (2109, 1.0, 0.43641847595400063),\n",
       " (2110, 1.0, 0.7962344290543661),\n",
       " (2111, 1.0, 0.4190447634094279),\n",
       " (2112, 1.0, 0.3789379716332743),\n",
       " (2113, 1.0, 2.5472430668224217),\n",
       " (2114, 1.0, 2.5795974055831628),\n",
       " (2115, 1.0, 2.4115937060247497),\n",
       " (2116, 1.0, 2.4656947632867943),\n",
       " (2117, 1.0, 2.640275949957747),\n",
       " (2118, 1.0, 2.614159957744646),\n",
       " (2119, 1.0, 0.6418870545214977),\n",
       " (2120, 1.0, 2.0527387129030346),\n",
       " (2121, 1.0, 2.266645784274402),\n",
       " (2122, 1.0, 2.2693792561426274),\n",
       " (2123, 1.0, 2.799497606166336),\n",
       " (2124, 1.0, -0.05359605990979328),\n",
       " (2125, 1.0, 2.583610457640202),\n",
       " (2126, 1.0, 2.333134655465465),\n",
       " (2127, 1.0, 2.620657296805884),\n",
       " (2128, 1.0, 1.0367250370845864),\n",
       " (2129, 1.0, 0.380756226087776),\n",
       " (2130, 1.0, 1.737130219407888),\n",
       " (2131, 1.0, 0.2742232326617712),\n",
       " (2132, 1.0, 0.5489221062705288),\n",
       " (2133, 1.0, 1.5979671487569442),\n",
       " (2134, 1.0, 2.581277890773095),\n",
       " (2135, 1.0, 0.08400369848522767),\n",
       " (2136, 1.0, 1.8610256309645288),\n",
       " (2137, 1.0, 2.401908529912806),\n",
       " (2138, 1.0, 0.07025281705691579),\n",
       " (2139, 1.0, 2.4229714280561145),\n",
       " (2140, 1.0, 2.595870370512187),\n",
       " (2141, 1.0, 2.502387685961164),\n",
       " (2142, 1.0, 0.7918207567479149),\n",
       " (2143, 1.0, 2.006452647734976),\n",
       " (2144, 1.0, 0.41715237785234743),\n",
       " (2145, 1.0, 2.1806957996743996),\n",
       " (2146, 1.0, -0.19335262090220776),\n",
       " (2147, 1.0, 2.478342139465713),\n",
       " (2148, 1.0, 2.4311640499590834),\n",
       " (2149, 1.0, 0.005144516676202111),\n",
       " (2150, 1.0, 2.837726870995867),\n",
       " (2151, 1.0, 2.496867969630794),\n",
       " (2152, 1.0, 0.5782949906019411),\n",
       " (2153, 1.0, 0.6470226436445139),\n",
       " (2154, 1.0, 2.5024949700257646),\n",
       " (2155, 1.0, 2.3958801704923567),\n",
       " (2156, 1.0, 2.665404543623128),\n",
       " (2157, 1.0, 1.975762207987654),\n",
       " (2158, 1.0, 2.1214242359011197),\n",
       " (2159, 1.0, 2.5413191863734115),\n",
       " (2160, 1.0, 2.681180260121933),\n",
       " (2161, 1.0, 1.2518385789689317),\n",
       " (2162, 1.0, 2.0307080151814336),\n",
       " (2163, 1.0, 2.7240955127087183),\n",
       " (2164, 1.0, 2.5042794864508786),\n",
       " (2165, 1.0, 0.8298543588542087),\n",
       " (2166, 1.0, 0.44289366460822804),\n",
       " (2167, 1.0, 2.149512947360174),\n",
       " (2168, 1.0, 2.708089585418203),\n",
       " (2169, 1.0, 2.363388861970952),\n",
       " (2170, 1.0, 2.0990717359107025),\n",
       " (2171, 1.0, 2.438202535487946),\n",
       " (2172, 1.0, 1.2438257943701994),\n",
       " (2173, 1.0, -0.2459427280011296),\n",
       " (2174, 1.0, 1.8990165668745065),\n",
       " (2175, 1.0, 1.111879053562513),\n",
       " (2176, 1.0, 2.6975870280963488),\n",
       " (2177, 1.0, 0.3685293236383803),\n",
       " (2178, 1.0, -0.03424856919969874),\n",
       " (2179, 1.0, 0.7086292612891711),\n",
       " (2180, 1.0, 2.5986785903477196),\n",
       " (2181, 1.0, 0.5637400212737388),\n",
       " (2182, 1.0, 2.323661165738379),\n",
       " (2183, 1.0, 2.033121117515264),\n",
       " (2184, 1.0, 2.366087333869156),\n",
       " (2185, 1.0, 1.8414685130810402),\n",
       " (2186, 1.0, 0.8436159577406022),\n",
       " (2187, 1.0, 1.0623815186569774),\n",
       " (2188, 1.0, 0.3136822295175891),\n",
       " (2189, 1.0, 0.6505458797923428),\n",
       " (2190, 1.0, 2.811169522505183),\n",
       " (2191, 1.0, 0.6924089618864128),\n",
       " (2192, 1.0, 0.3892299485762951),\n",
       " (2193, 1.0, 2.4314020231444604),\n",
       " (2194, 1.0, -0.18975353659225475),\n",
       " (2195, 1.0, 0.13447576440298056),\n",
       " (2196, 1.0, 2.5571571096738483),\n",
       " (2197, 1.0, 2.4123183846054044),\n",
       " (2198, 1.0, 3.167741070296766),\n",
       " (2199, 1.0, 2.233536636145971),\n",
       " (2200, 1.0, 2.373871915906881),\n",
       " (2201, 1.0, 1.8267863736613752),\n",
       " (2202, 1.0, -0.08369663125601372),\n",
       " (2203, 1.0, 0.8381113231974539),\n",
       " (2204, 1.0, 0.5244769461353694),\n",
       " (2205, 1.0, 2.3216383989723153),\n",
       " (2206, 1.0, 0.5072867820525341),\n",
       " (2207, 1.0, 0.12407926657717658),\n",
       " (2208, 1.0, 2.3107151568297337),\n",
       " (2209, 1.0, 2.3085836835120235),\n",
       " (2210, 1.0, 1.5009883903354615),\n",
       " (2211, 1.0, 1.7050171654192645),\n",
       " (2212, 1.0, 0.9482766325170342),\n",
       " (2213, 1.0, 2.3656842534915077),\n",
       " (2214, 1.0, 0.48458141366769936),\n",
       " (2215, 1.0, 0.48037877814487684),\n",
       " (2216, 1.0, 0.34659058768814577),\n",
       " (2217, 1.0, 2.4081766998830636),\n",
       " (2218, 1.0, 2.1659224743158036),\n",
       " (2219, 1.0, 2.1328737492069982),\n",
       " (2220, 1.0, 2.0295521271747394),\n",
       " (2221, 1.0, 2.7264783957631966),\n",
       " (2222, 1.0, 2.760779084400431),\n",
       " (2223, 1.0, -0.2058015565867451),\n",
       " (2224, 1.0, 2.475124554400613),\n",
       " (2225, 1.0, 2.065421296089198),\n",
       " (2226, 1.0, -0.37722142026667643),\n",
       " (2227, 1.0, 0.5156651922198631),\n",
       " (2228, 1.0, 1.1159987745138094),\n",
       " (2229, 1.0, 1.8893187976934656),\n",
       " (4066, 2.0, 2.8900374705384895),\n",
       " (4067, 2.0, 2.9529314940061644),\n",
       " (4068, 2.0, 2.471037535019845),\n",
       " (4069, 2.0, 0.5428149645957207),\n",
       " (4070, 2.0, 0.8558376135492195),\n",
       " (4071, 2.0, 0.9221747530156529),\n",
       " (4072, 2.0, 2.1987096967983346),\n",
       " (4073, 2.0, 2.415195313509655),\n",
       " (4074, 2.0, 0.5865044914154949),\n",
       " (4075, 2.0, 2.4937479123413246),\n",
       " (4076, 2.0, 2.125925951564978),\n",
       " (4077, 2.0, 0.5153105385128677),\n",
       " (4078, 2.0, 2.871039693850403),\n",
       " (4079, 2.0, 0.7882791198763821),\n",
       " (4080, 2.0, 2.6960721124395577),\n",
       " (4081, 2.0, 0.384076141364763),\n",
       " (4082, 2.0, 2.215442296326103),\n",
       " (4083, 2.0, 0.79868324109139),\n",
       " (4084, 2.0, 1.052016528955272),\n",
       " (4085, 2.0, 2.9712310004992584),\n",
       " (4086, 2.0, 2.4581405484349),\n",
       " (4087, 2.0, 1.984263404429067),\n",
       " (4088, 2.0, 0.19338494058449043),\n",
       " (4089, 2.0, 0.028699833843172293),\n",
       " (4090, 2.0, 2.004585653934837),\n",
       " (4091, 2.0, 1.9087128304380365),\n",
       " (4092, 2.0, 0.8009882161405368),\n",
       " (4093, 2.0, 2.7091488661468848),\n",
       " (4094, 2.0, 2.140078770219004),\n",
       " (4095, 2.0, 1.7983968645760695),\n",
       " (4096, 2.0, 0.9510910258384566),\n",
       " (4097, 2.0, 2.1513792134274485),\n",
       " (4098, 2.0, 1.0379597443444846),\n",
       " (4099, 2.0, 0.5162425274334996),\n",
       " (4100, 2.0, 2.465474433061877),\n",
       " (4101, 2.0, 1.823107741553044),\n",
       " (4102, 2.0, 0.7640254610447897),\n",
       " (4103, 2.0, 2.7149249466471197),\n",
       " (4104, 2.0, 0.6838930710083848),\n",
       " (4105, 2.0, 1.9486169232906063),\n",
       " (4106, 2.0, 0.7963724605583196),\n",
       " (4107, 2.0, 1.1822885106273708),\n",
       " (4108, 2.0, 0.6630082213179771),\n",
       " (4109, 2.0, 2.816702094368608),\n",
       " (4110, 2.0, 0.6142464764586695),\n",
       " (4111, 2.0, 2.3364257016043597),\n",
       " (4112, 2.0, 2.056481173061044),\n",
       " (4113, 2.0, 2.0563680848345873),\n",
       " (4114, 2.0, 2.4952233004657747),\n",
       " (4115, 2.0, 2.633063834184667),\n",
       " (4116, 2.0, 2.305884191389336),\n",
       " (4117, 2.0, 0.524632767427822),\n",
       " (4118, 2.0, 2.31143885306563),\n",
       " (4119, 2.0, 2.562218796263646),\n",
       " (4120, 2.0, 2.4000723380889646),\n",
       " (4121, 2.0, 2.240843318289191),\n",
       " (4122, 2.0, 2.3319703471823923),\n",
       " (4123, 2.0, 2.2518607909572568),\n",
       " (4124, 2.0, 2.326365164749895),\n",
       " (4125, 2.0, 2.5059315405728313),\n",
       " (4126, 2.0, 2.112424253598523),\n",
       " (4127, 2.0, 2.419405141713025),\n",
       " (4128, 2.0, 0.16343084178370945),\n",
       " (4129, 2.0, 0.530814443413168),\n",
       " (4130, 2.0, 0.8560749709783954),\n",
       " (4131, 2.0, 2.166421736751489),\n",
       " (4132, 2.0, 2.488267833798295),\n",
       " (4133, 2.0, 2.234651622848136),\n",
       " (4134, 2.0, 2.9504120616652045),\n",
       " (4135, 2.0, 0.570590072771799),\n",
       " (4136, 2.0, 2.567234149243028),\n",
       " (4137, 2.0, 0.1197342323639909),\n",
       " (4138, 2.0, 2.338061330392183),\n",
       " (4139, 2.0, 2.714527916962507),\n",
       " (4140, 2.0, 2.8977790476124334),\n",
       " (4141, 2.0, 1.606816938744367),\n",
       " (4142, 2.0, 2.7679988168348455),\n",
       " (4143, 2.0, 1.7863427351024597),\n",
       " (4144, 2.0, 0.6049239751222317),\n",
       " (4145, 2.0, 2.494248421168768),\n",
       " (4146, 2.0, 0.9884875388850216),\n",
       " (4147, 2.0, 0.343764339539503),\n",
       " (4148, 2.0, 2.308368090258394),\n",
       " (4149, 2.0, 2.339055127957902),\n",
       " (4150, 2.0, 0.826937703008178),\n",
       " (4151, 2.0, 0.5541694798904762),\n",
       " (4152, 2.0, 1.015556828174267),\n",
       " (4153, 2.0, 0.792519099968993),\n",
       " (4154, 2.0, 1.3465531645003848),\n",
       " (4155, 2.0, 0.5184684513193744),\n",
       " (4156, 2.0, 2.0692699371250964),\n",
       " (4157, 2.0, 2.9800433314378125),\n",
       " (4158, 2.0, 1.5987341049648123),\n",
       " (4159, 2.0, 2.359463056851325),\n",
       " (4160, 2.0, 2.8686867991982004),\n",
       " (4161, 2.0, 0.8723859555191733),\n",
       " (4162, 2.0, 2.4394445961412763),\n",
       " (4163, 2.0, 0.027882673993162427),\n",
       " (4164, 2.0, 2.0419994144176132),\n",
       " (4165, 2.0, 0.5096790549739103),\n",
       " (4166, 2.0, 1.881953562073259),\n",
       " (4167, 2.0, 1.3382872505524677),\n",
       " (4168, 2.0, 2.4032973757733673),\n",
       " (4169, 2.0, 2.9999482246980227),\n",
       " (4170, 2.0, 0.382572213695128),\n",
       " (4171, 2.0, 2.485034096635492),\n",
       " (4172, 2.0, 0.6157307292966093),\n",
       " (4173, 2.0, 1.012261168355606),\n",
       " (4174, 2.0, 2.5551886434843567),\n",
       " (4175, 2.0, 0.6831157201127853),\n",
       " (4176, 2.0, 1.7169802332807418),\n",
       " (4177, 2.0, 0.7222174239743591),\n",
       " (4178, 2.0, 1.991536146728387),\n",
       " (4179, 2.0, 1.036210594330576),\n",
       " (4180, 2.0, 2.3854910239040032),\n",
       " (4181, 2.0, 0.7889006467616655),\n",
       " (4182, 2.0, 2.745722655364539),\n",
       " (4183, 2.0, 0.4910880573779064),\n",
       " (4184, 2.0, 2.781397496850469),\n",
       " (4185, 2.0, 2.613303411396935),\n",
       " (4186, 2.0, 0.4761314068599901),\n",
       " (4187, 2.0, 2.8484064396680413),\n",
       " (4188, 2.0, 2.5173689415153904),\n",
       " (4189, 2.0, 0.27219985461936747),\n",
       " (4190, 2.0, 0.4513383535425852),\n",
       " (4191, 2.0, 0.06791562248996434),\n",
       " (4192, 2.0, 2.216971310715747),\n",
       " (4193, 2.0, 0.7716181218532133),\n",
       " (4194, 2.0, 0.4037339168833627),\n",
       " (4195, 2.0, 2.5737211689505872),\n",
       " (4196, 2.0, 0.6589412757089692),\n",
       " (4197, 2.0, 2.5161637552668843),\n",
       " (4198, 2.0, 2.489581218952214),\n",
       " (4199, 2.0, 0.746096243038079),\n",
       " (4200, 2.0, 1.949747682829505),\n",
       " (4201, 2.0, 0.9308008959224738),\n",
       " (4202, 2.0, 0.689706345247049),\n",
       " (4203, 2.0, 0.5927013304696389),\n",
       " (4204, 2.0, 2.670500150995393),\n",
       " (4205, 2.0, 2.7293303773309923),\n",
       " (4206, 2.0, 0.8291987180485051),\n",
       " (4207, 2.0, 2.780450172160433),\n",
       " (4208, 2.0, 0.44148987612171386),\n",
       " (4209, 2.0, 2.222981325000632),\n",
       " (4210, 2.0, 0.7085853861342771),\n",
       " (4211, 2.0, 2.8848054023334226),\n",
       " (4212, 2.0, 0.8284118049747081),\n",
       " (4213, 2.0, 2.557556405761112),\n",
       " (4214, 2.0, 0.1971795466811016),\n",
       " (4215, 2.0, 2.741393634625255),\n",
       " (4216, 2.0, 2.213939824589143),\n",
       " (4217, 2.0, 0.5445912356084809),\n",
       " (4218, 2.0, 2.489545749842056),\n",
       " (4219, 2.0, 0.7724014272713471),\n",
       " (4220, 2.0, 2.4472134224883244),\n",
       " (4221, 2.0, 2.3468843558749373),\n",
       " (4222, 2.0, 2.0017424935799633),\n",
       " (4223, 2.0, 2.2596362491682154),\n",
       " (4224, 2.0, 0.6497877247443082),\n",
       " (4225, 2.0, 0.6616678518241899),\n",
       " (4226, 2.0, 2.5032824371100397),\n",
       " (4227, 2.0, 0.6200725800351153),\n",
       " (4228, 2.0, 1.984473240431746),\n",
       " (4229, 2.0, 0.558587034621104),\n",
       " (4230, 2.0, 2.5802226422754453),\n",
       " (4231, 2.0, 2.57186940673153),\n",
       " (4232, 2.0, 2.616488630457358),\n",
       " (4233, 2.0, 0.6121419183067749),\n",
       " (4234, 2.0, 2.4066656426083157),\n",
       " (4235, 2.0, 2.645923561228253),\n",
       " (4236, 2.0, 0.5740152874940989),\n",
       " (4237, 2.0, 2.6536308062829543),\n",
       " (4238, 2.0, 0.3869265741688462),\n",
       " (4239, 2.0, 2.2973850383752086),\n",
       " (4240, 2.0, 2.2049774822565933),\n",
       " (4241, 2.0, 2.0263593199403656),\n",
       " (4242, 2.0, 0.9434063326048971),\n",
       " (4243, 2.0, 2.378885633609691),\n",
       " (4244, 2.0, 1.9876225454994843),\n",
       " (4245, 2.0, 2.600385668098307),\n",
       " (4246, 2.0, 2.3899715168173437),\n",
       " (4247, 2.0, 0.21495786690063448),\n",
       " (4248, 2.0, 2.275564446810555),\n",
       " (4249, 2.0, 2.303004259810123),\n",
       " (4250, 2.0, 2.481409502550272),\n",
       " (4251, 2.0, 2.5606232133964477),\n",
       " (4252, 2.0, 2.683525756658139),\n",
       " (4253, 2.0, 2.1503317316569195),\n",
       " (4254, 2.0, 2.90194326334555),\n",
       " (4255, 2.0, 2.156418806725573),\n",
       " (4256, 2.0, 0.5371398702727516),\n",
       " (4257, 2.0, 2.7539517367431743),\n",
       " (4258, 2.0, 2.5925278191644017),\n",
       " (4259, 2.0, 0.6397619052353322),\n",
       " (4260, 2.0, 2.608056604285521),\n",
       " (4261, 2.0, 0.5073840561522892),\n",
       " (4262, 2.0, 2.3901098569391066),\n",
       " (4263, 2.0, 2.828845303771153),\n",
       " (4264, 2.0, 2.120470928972036),\n",
       " (4265, 2.0, 2.6381740526147865),\n",
       " (4266, 2.0, 0.9218570216859271),\n",
       " (4267, 2.0, 2.3146367251080147),\n",
       " (4268, 2.0, 0.9470141267368903),\n",
       " (4269, 2.0, 0.7588737602962602),\n",
       " (4270, 2.0, 0.3573233183809146),\n",
       " (4271, 2.0, 0.8991786459078144),\n",
       " (4272, 2.0, 0.7020084442452008),\n",
       " (4273, 2.0, 0.41024055908679924),\n",
       " (4274, 2.0, 1.1203523489644407),\n",
       " (4275, 2.0, -0.20049032436142644),\n",
       " (4276, 2.0, 2.5235315149754824),\n",
       " (4277, 2.0, 2.9648058045343126),\n",
       " (4278, 2.0, 1.2043168329105658),\n",
       " (4279, 2.0, 1.0107584632604114),\n",
       " (4280, 2.0, 2.5407686903472433),\n",
       " (4281, 2.0, 2.509270100031604),\n",
       " (4282, 2.0, 2.8524282305388455),\n",
       " (4283, 2.0, 0.48262808989332184),\n",
       " (4284, 2.0, 2.692817195932833),\n",
       " (4285, 2.0, 0.600382706842206),\n",
       " (4286, 2.0, 2.3351341478032617),\n",
       " (4287, 2.0, 2.2533103959201717),\n",
       " (4288, 2.0, 0.11541737605133744),\n",
       " (4289, 2.0, 2.434386400771359),\n",
       " (4290, 2.0, 2.1614743831458134),\n",
       " (4291, 2.0, 2.4570606738569394),\n",
       " (4292, 2.0, 2.579728783306682),\n",
       " (4293, 2.0, 2.410074374689954),\n",
       " (4294, 2.0, 2.8179871349677033),\n",
       " (4295, 2.0, 2.3472202207404016),\n",
       " (4296, 2.0, 1.4749459132975984),\n",
       " (4297, 2.0, 2.249318981743457),\n",
       " (4298, 2.0, 2.411032898765696),\n",
       " (4299, 2.0, 2.487686869949162),\n",
       " (4300, 2.0, 2.3403461892259076),\n",
       " (4301, 2.0, 0.4293039017053246),\n",
       " (4302, 2.0, 0.8517872288701878),\n",
       " (4303, 2.0, 1.9114504544553248),\n",
       " (4304, 2.0, 1.07712496722461),\n",
       " (4305, 2.0, 0.8449436814088673),\n",
       " (4306, 2.0, 3.0516608839741024),\n",
       " (4307, 2.0, 0.6653747218709735),\n",
       " (4308, 2.0, 2.756214865964517),\n",
       " (4309, 2.0, 2.5290971586825712),\n",
       " (4310, 2.0, 0.5305989201440492),\n",
       " (4311, 2.0, 0.6066794476828994),\n",
       " (4312, 2.0, 1.2948184527236186),\n",
       " (4313, 2.0, 2.2544086058949815),\n",
       " (4314, 2.0, 0.6253514154062774),\n",
       " (4315, 2.0, 2.4631494986341242),\n",
       " (4316, 2.0, 2.6432805548738045),\n",
       " (4317, 2.0, 2.2912119989681017),\n",
       " (4318, 2.0, 0.030109252428664805),\n",
       " (4319, 2.0, 0.5370379561271711),\n",
       " (4320, 2.0, 2.493401790811264),\n",
       " (4321, 2.0, 0.557291368298116),\n",
       " (4322, 2.0, 0.8149762103651186),\n",
       " (4323, 2.0, 1.8034907142696615),\n",
       " (4324, 2.0, 2.557304317672944),\n",
       " (4325, 2.0, 1.2060567696931996),\n",
       " (4326, 2.0, 1.9474359431970898),\n",
       " (4327, 2.0, 2.2766353727208273),\n",
       " (4328, 2.0, 2.558258702754624),\n",
       " (4329, 2.0, 0.9715004362107723),\n",
       " (4330, 2.0, 0.8664662137028952),\n",
       " (4331, 2.0, 2.4826505696712164),\n",
       " (4332, 2.0, 2.4070292579973023),\n",
       " (4333, 2.0, 2.8367974080717255),\n",
       " (4334, 2.0, 0.21799567134957565),\n",
       " (4335, 2.0, 2.350129817239076),\n",
       " (4336, 2.0, 1.1380939467545519),\n",
       " (4337, 2.0, 2.348276045030138),\n",
       " (4338, 2.0, 0.6340808950517902),\n",
       " (4339, 2.0, 0.910456021110974),\n",
       " (4340, 2.0, 2.4032766704956945),\n",
       " (4341, 2.0, 0.28509288073711725),\n",
       " (4342, 2.0, 1.2921705065833626),\n",
       " (4343, 2.0, 1.795555225122608),\n",
       " (4344, 2.0, 0.8026948090641639),\n",
       " (4345, 2.0, 2.700212158975167),\n",
       " (4346, 2.0, 2.345581124667749),\n",
       " (4347, 2.0, 2.0667358828230253),\n",
       " (4348, 2.0, 2.414966111331005),\n",
       " (4349, 2.0, 2.4030175469900756),\n",
       " (4350, 2.0, 0.6775161525559934),\n",
       " (4351, 2.0, 0.4114722627995181),\n",
       " (4352, 2.0, 2.509437732743834),\n",
       " (4353, 2.0, 1.0147970393785346),\n",
       " (4354, 2.0, 2.417054697996847),\n",
       " (4355, 2.0, 2.329001303934211),\n",
       " (4356, 2.0, 2.7902978241378973),\n",
       " (4357, 2.0, 1.9875321619042745),\n",
       " (4358, 2.0, 2.377201169709684),\n",
       " (4359, 2.0, 2.7647925602213865),\n",
       " (4360, 2.0, 0.8641732770017032),\n",
       " (4361, 2.0, 0.9236958172998633),\n",
       " (4362, 2.0, 2.554099051098221),\n",
       " (4363, 2.0, 1.5684817832245537),\n",
       " (4364, 2.0, 0.1540616485635974),\n",
       " (4365, 2.0, 2.5560777133667183),\n",
       " (4366, 2.0, 2.0730588822451503),\n",
       " (4367, 2.0, 2.645019393427978),\n",
       " (4368, 2.0, 2.345964772457628),\n",
       " (4369, 2.0, 0.5525433065048998),\n",
       " (4370, 2.0, 2.5460468859417844),\n",
       " (4371, 2.0, 2.1730291028259536),\n",
       " (4372, 2.0, 2.343853387972499),\n",
       " (4373, 2.0, 2.1152498239419897),\n",
       " (4374, 2.0, 2.533387981581923),\n",
       " (4375, 2.0, 1.5663810108632943),\n",
       " (4376, 2.0, 0.2837211546549865),\n",
       " (4377, 2.0, 0.1901080317544656),\n",
       " (4378, 2.0, 2.1223208142544725),\n",
       " (4379, 2.0, 2.615415531626934),\n",
       " (4380, 2.0, 2.658925347693977),\n",
       " (4381, 2.0, 2.269866699932051),\n",
       " (4382, 2.0, 2.330358909814968),\n",
       " (4383, 2.0, 1.7487715829922446),\n",
       " (4384, 2.0, 0.8534336086640267),\n",
       " (4385, 2.0, 2.1447362321062986),\n",
       " (4386, 2.0, 2.423456148873479),\n",
       " (4387, 2.0, 2.0844927874529136),\n",
       " (4388, 2.0, 1.8416048833572152),\n",
       " (4389, 2.0, 2.6392579988270213),\n",
       " (4390, 2.0, 2.0440476082676846),\n",
       " (4391, 2.0, 2.693243094402037),\n",
       " (4392, 2.0, 2.420887934526462),\n",
       " (4393, 2.0, 2.623539231404843),\n",
       " (4394, 2.0, 1.8476573440633817),\n",
       " (4395, 2.0, 2.270422833741889),\n",
       " (4396, 2.0, 0.972738636171684),\n",
       " (4397, 2.0, 0.7270779899595778),\n",
       " (4398, 2.0, 2.917053692118992),\n",
       " (4399, 2.0, 2.3376752600601662),\n",
       " (4400, 2.0, 2.1853541692366165),\n",
       " (4401, 2.0, 0.17619747099698327),\n",
       " (4402, 2.0, 2.1830282862084833),\n",
       " (4403, 2.0, 1.1004062600084035),\n",
       " (4404, 2.0, 1.8587454547146796),\n",
       " (4405, 2.0, 2.010533637010475),\n",
       " (4406, 2.0, 1.0523508100763184),\n",
       " (4407, 2.0, 2.4453519770384213),\n",
       " (4408, 2.0, 0.9169806362684209),\n",
       " (4409, 2.0, 2.8813954475586887),\n",
       " (4410, 2.0, 2.370085077874094),\n",
       " (4411, 2.0, 2.2907312189405378),\n",
       " (4412, 2.0, 2.248770240236945),\n",
       " (4413, 2.0, 0.9428152429293443),\n",
       " (4414, 2.0, 2.3798480411546743),\n",
       " (4415, 2.0, 2.6565471240307743),\n",
       " (4416, 2.0, 1.0248452966816564),\n",
       " (4417, 2.0, 1.05683374700403),\n",
       " (4418, 2.0, 0.710492434504918),\n",
       " (4419, 2.0, 2.4521765053860682),\n",
       " (4420, 2.0, 2.2763526484511396),\n",
       " (4421, 2.0, 0.9895582797709435),\n",
       " (4422, 2.0, 0.2618263699091252),\n",
       " (4423, 2.0, 0.49722474004577305),\n",
       " (4424, 2.0, 2.6648507872734286),\n",
       " (4425, 2.0, 0.2324728537056683),\n",
       " (4426, 2.0, 2.4657358731304306),\n",
       " (4427, 2.0, 2.279254045723499),\n",
       " (4428, 2.0, 2.6752785639661405),\n",
       " (4429, 2.0, 2.7945549402971306),\n",
       " (4430, 2.0, 2.9379610073186395),\n",
       " (4431, 2.0, 2.349725773896767),\n",
       " (4432, 2.0, 1.3281621589975996),\n",
       " (4433, 2.0, 2.30045080537941),\n",
       " (4434, 2.0, 2.3013523032779766),\n",
       " (4435, 2.0, 2.7690619216592256),\n",
       " (4436, 2.0, 2.1501443627999506),\n",
       " (4437, 2.0, 2.4224469015146286),\n",
       " (4438, 2.0, 0.5702190357424596),\n",
       " (4439, 2.0, 2.0391947849569587),\n",
       " (4440, 2.0, 1.1448365792646285),\n",
       " (4441, 2.0, 2.0363277623178324),\n",
       " (4442, 2.0, 0.7513315321791841),\n",
       " (4443, 2.0, 2.1342721594562253),\n",
       " (4444, 2.0, 2.7927442774316016),\n",
       " (4445, 2.0, 2.6386971612004464),\n",
       " (4446, 2.0, 2.5722834016800196),\n",
       " (4447, 2.0, 0.44718328251811357),\n",
       " (4448, 2.0, 2.3361684619263308),\n",
       " (4449, 2.0, 2.535280423451466),\n",
       " (4450, 2.0, 0.006012218695815015),\n",
       " (4451, 2.0, 1.9212896831137658),\n",
       " (4452, 2.0, 2.0259252680281357),\n",
       " (4453, 2.0, 0.3566325018739905),\n",
       " (4454, 2.0, 2.4379491718323822),\n",
       " (4455, 2.0, 2.7684398688936196),\n",
       " (4456, 2.0, 2.331275376773498),\n",
       " (4457, 2.0, 2.4476564441779187),\n",
       " (4458, 2.0, 0.41184997641063276),\n",
       " (4459, 2.0, 2.071204591218775),\n",
       " (4460, 2.0, 2.075266169229821),\n",
       " (4461, 2.0, 2.245738283586743),\n",
       " (4462, 2.0, 2.0142535029092308),\n",
       " (4463, 2.0, 2.2565720468621144),\n",
       " (4464, 2.0, 0.13192168962392592),\n",
       " (4465, 2.0, 2.3778019041112195),\n",
       " (4466, 2.0, 2.2674148657413955),\n",
       " (4467, 2.0, 3.093742028218801),\n",
       " (4468, 2.0, 2.287266408981447),\n",
       " (4469, 2.0, 2.769862902827126),\n",
       " (4470, 2.0, 0.3541089889586784),\n",
       " (4471, 2.0, 0.6002986134375755),\n",
       " (4472, 2.0, 2.0227361322795394),\n",
       " (4473, 2.0, 0.8789656150245978),\n",
       " (4474, 2.0, 1.2614377401343337),\n",
       " (4475, 2.0, 0.38770890134392366),\n",
       " (4476, 2.0, 2.0779053987585443),\n",
       " (4477, 2.0, 2.9069100076651466),\n",
       " (4478, 2.0, 1.6814284771263674),\n",
       " (4479, 2.0, 0.03998045554602141),\n",
       " (4480, 2.0, 2.193795894750004),\n",
       " (4481, 2.0, 2.307397404311717),\n",
       " (4482, 2.0, 2.1531985295995284),\n",
       " (4483, 2.0, 2.227666203649513),\n",
       " (4484, 2.0, 0.6865156793487167),\n",
       " (4485, 2.0, 0.24909714580483583),\n",
       " (4486, 2.0, 0.5532563545609157),\n",
       " (4487, 2.0, 0.4900372622585112),\n",
       " (4488, 2.0, 2.4808590352460067),\n",
       " (4489, 2.0, 1.8980313621451275),\n",
       " (4490, 2.0, 2.8579992487738304),\n",
       " (4491, 2.0, 0.902875232808298),\n",
       " (6809, 3.0, 1.0170789376013092),\n",
       " (6812, 3.0, 0.5655933187944302),\n",
       " (6813, 3.0, 1.003813073293644),\n",
       " (6814, 3.0, 0.85577879820054),\n",
       " (6815, 3.0, 0.6651923979770207),\n",
       " (6816, 3.0, 1.6864808462558845),\n",
       " (6817, 3.0, 1.7748186902493148),\n",
       " (6818, 3.0, 1.8990609545786656),\n",
       " (6820, 3.0, 0.327599530726942),\n",
       " (6821, 3.0, 1.7743067083761626),\n",
       " (6822, 3.0, 0.650773654344953),\n",
       " (6823, 3.0, 0.7797813480597757),\n",
       " (6824, 3.0, 0.9790468146992594),\n",
       " (6825, 3.0, 0.918932458249286),\n",
       " (6826, 3.0, 0.9938096981378121),\n",
       " (6827, 3.0, 0.33852882086527764),\n",
       " (6829, 3.0, 0.8479726268014681),\n",
       " ...]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1673</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1674</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1676</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1678</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1679</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1682</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1683</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1685</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1686</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1687</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1688</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1689</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1690</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1694</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1695</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1696</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1698</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1699</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1700</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>28105</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>28106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>28107</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>28108</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>28109</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>28110</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>28111</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>28112</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>28113</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>28114</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>28115</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>28116</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>28117</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>28118</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5208</th>\n",
       "      <td>28119</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>28120</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5210</th>\n",
       "      <td>28121</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5211</th>\n",
       "      <td>28122</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5212</th>\n",
       "      <td>28123</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>28124</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>28125</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>28126</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5216</th>\n",
       "      <td>28127</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5217</th>\n",
       "      <td>28128</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5218</th>\n",
       "      <td>28129</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5219</th>\n",
       "      <td>28130</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>28131</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5221</th>\n",
       "      <td>28132</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222</th>\n",
       "      <td>28133</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223</th>\n",
       "      <td>28134</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5224 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  essay_set  essay_score\n",
       "0      1673        1.0            1\n",
       "1      1674        1.0            1\n",
       "2      1675        1.0            3\n",
       "3      1676        1.0            0\n",
       "4      1677        1.0            0\n",
       "5      1678        1.0            2\n",
       "6      1679        1.0            0\n",
       "7      1680        1.0            1\n",
       "8      1681        1.0            2\n",
       "9      1682        1.0            2\n",
       "10     1683        1.0            2\n",
       "11     1684        1.0            1\n",
       "12     1685        1.0            0\n",
       "13     1686        1.0            2\n",
       "14     1687        1.0            1\n",
       "15     1688        1.0            2\n",
       "16     1689        1.0            2\n",
       "17     1690        1.0            0\n",
       "18     1691        1.0            1\n",
       "19     1692        1.0            2\n",
       "20     1693        1.0            1\n",
       "21     1694        1.0            0\n",
       "22     1695        1.0            0\n",
       "23     1696        1.0            1\n",
       "24     1697        1.0            1\n",
       "25     1698        1.0            0\n",
       "26     1699        1.0            3\n",
       "27     1700        1.0            3\n",
       "28     1701        1.0            3\n",
       "29     1702        1.0            2\n",
       "...     ...        ...          ...\n",
       "5194  28105       10.0            2\n",
       "5195  28106       10.0            1\n",
       "5196  28107       10.0            2\n",
       "5197  28108       10.0            2\n",
       "5198  28109       10.0            1\n",
       "5199  28110       10.0            2\n",
       "5200  28111       10.0            1\n",
       "5201  28112       10.0            1\n",
       "5202  28113       10.0            1\n",
       "5203  28114       10.0            2\n",
       "5204  28115       10.0            1\n",
       "5205  28116       10.0            2\n",
       "5206  28117       10.0            1\n",
       "5207  28118       10.0            2\n",
       "5208  28119       10.0            2\n",
       "5209  28120       10.0            2\n",
       "5210  28121       10.0            1\n",
       "5211  28122       10.0            1\n",
       "5212  28123       10.0            2\n",
       "5213  28124       10.0            1\n",
       "5214  28125       10.0            1\n",
       "5215  28126       10.0            1\n",
       "5216  28127       10.0            2\n",
       "5217  28128       10.0            2\n",
       "5218  28129       10.0            2\n",
       "5219  28130       10.0            1\n",
       "5220  28131       10.0            2\n",
       "5221  28132       10.0            1\n",
       "5222  28133       10.0            1\n",
       "5223  28134       10.0            0\n",
       "\n",
       "[5224 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ = pd.DataFrame(l,columns=['id','essay_set','essay_score'])\n",
    "res_n = list(map(lambda x : round(x) , list(res_['essay_score'])))\n",
    "res_['essay_score'] = res_n\n",
    "res_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_.to_csv('/home/prashant/incedo/sen_level_embed_tfidf_todu_fet_no_embed__added_regression.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['EssayText'][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operator\n",
    "# l  = [[9.99690792e-01 ,1, 1.51614726e-07, 4.23026149e-13],[1,2,3,4]]\n",
    "# index, value = max(enumerate(l), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_internal = list(map(lambda x : max(enumerate(x), key=operator.itemgetter(1))[0],l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(\"\")\n",
    "# cnt = 0\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     cnt +=1\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#78.7\n",
    "\n",
    "# 'num_leaves': 15,\n",
    "# 'max_bin': 100,\n",
    "# 'num_class':num_cla,\n",
    "# 'min_data_in_leaf': 50,\n",
    "# 'learning_rate': 0.01,\n",
    "# 'min_sum_hessian_in_leaf': 0.000446,\n",
    "# 'bagging_fraction': 0.55,\n",
    "# 'bagging_freq': 5,\n",
    "# 'max_depth': 14,\n",
    "# 'save_binary': True,\n",
    "# 'seed': 31452,\n",
    "# 'feature_fraction_seed': 31415,\n",
    "# 'feature_fraction': 0.51,\n",
    "# 'bagging_seed': 31415,\n",
    "# 'drop_seed': 31415,\n",
    "# 'data_random_seed': 31415,\n",
    "# 'objective': 'multiclass',\n",
    "# 'boosting_type': 'gbdt',\n",
    "# 'verbose': 1,htpo\n",
    "# 'metric': 'multi_logloss',\n",
    "\n",
    "# 'is_unbalance': False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - '/home/prashant/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/share/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/p3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/p3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/brown.zip/brown/\u001b[0m\n\n  Searched in:\n    - '/home/prashant/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/share/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4c2a2af48ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mword_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspelling_mistakes_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/p3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/p3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/p3/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/p3/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - '/home/prashant/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/share/nltk_data'\n    - '/home/prashant/miniconda2/envs/p3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "word_set = set(word_list)\n",
    "\n",
    "def spelling_mistakes_check(sentences):\n",
    "    for i in sentences:\n",
    "        l = []\n",
    "        cnt = 0\n",
    "        words = i.split(' ')\n",
    "        for w in words:\n",
    "            if w not in word_set:\n",
    "                cnt +=1\n",
    "        l.append(cnt)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/prashant/Downloads/essay_scoring-master/stanford-corenlp-full-2018-10-05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
