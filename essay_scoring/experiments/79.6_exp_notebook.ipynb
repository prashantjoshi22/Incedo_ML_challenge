{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def Xgboost(x_train, y_train, max_depth=17, n_estimators=100, learning_rate=0.1961):\n",
    "    gbm = xgb.XGBClassifier(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "    gbm.fit(x_train, y_train)\n",
    "   \n",
    "    return gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "stop=[]\n",
    "stop.extend(stopwords.words('english'))\n",
    "from textblob import TextBlob\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/prashant/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 17:33:45.575735 140166900168448 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 17:33:45.895883 140166900168448 monitored_session.py:222] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 17:33:46.757628 140166900168448 session_manager.py:491] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0427 17:33:47.960306 140166900168448 session_manager.py:493] Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "word_list = brown.words()\n",
    "word_set = set(word_list)\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment(data):\n",
    "    l = []\n",
    "    for t in data:\n",
    "        blob = TextBlob(t)\n",
    "        if len(t) ==0:\n",
    "            l.append(0)\n",
    "            continue\n",
    "        for sentence in blob.sentences:\n",
    "            l.append(sentence.sentiment.polarity)\n",
    "    return l\n",
    "\n",
    "\n",
    "def spelling_mistakes_check(sentences):\n",
    "    l = []\n",
    "    for i in sentences:\n",
    "        \n",
    "        cnt = 0\n",
    "        words = i.split(' ')\n",
    "        for w in words:\n",
    "            if w not in word_set:\n",
    "                cnt +=1\n",
    "        l.append(cnt)\n",
    "    return l\n",
    "\n",
    "def get_part_of_speech_tags(sentences):\n",
    "    all_sample_tags = []\n",
    "    for i in sentences:\n",
    "        dict_count = {}\n",
    "        result = (TextBlob(i))\n",
    "        for words, tag in result.tags:\n",
    "            if tag not in dict_count:\n",
    "                dict_count[tag] = 1\n",
    "            else:\n",
    "                dict_count[tag]  += 1\n",
    "        all_sample_tags.append(dict_count)\n",
    "    \n",
    "    cc = pd.DataFrame.from_dict(all_sample_tags)\n",
    "    cc.fillna(0,inplace=True)\n",
    "     \n",
    "    scaled_features = cc #StandardScaler().fit_transform(cc)\n",
    "    sc_ver = pd.DataFrame(scaled_features)\n",
    "    sc_ver.columns = cc.columns\n",
    "    return sc_ver\n",
    "\n",
    "\n",
    "def get_sen_len(sentences):\n",
    "    sen_len = []\n",
    "    for i in sentences:\n",
    "        i = len(i.split(' '))\n",
    "        sen_len.append(i)\n",
    "    return sen_len\n",
    "\n",
    "def preprocess_data(s):\n",
    "    s = s.lower()\n",
    "    data = re.sub(r'[^\\x00-\\x7F]+', ' ', s)\n",
    "    final_str = data.translate(str.maketrans('', '', string.punctuation))\n",
    "    filter_str = final_str.translate(str.maketrans('', '', digits))\n",
    "    nltk_tokens = nltk.word_tokenize(filter_str)\n",
    "    #Next find the roots of the word\n",
    "    str_= ''\n",
    "    for w in nltk_tokens:\n",
    "\n",
    "        if w not in stop:\n",
    "            str_ += ' '  + (lemmatizer.lemmatize(w))\n",
    "    \n",
    "    return str_.strip()\n",
    "\n",
    "def get_ner_tags(sentences):\n",
    "    all_sample_tags = []\n",
    "    for i in sentences:\n",
    "        dict_count = {}\n",
    "        doc = nlp(i)\n",
    "        result = ([(X.label_) for X in doc.ents])\n",
    "\n",
    "        for tag in result:\n",
    "            if tag not in dict_count:\n",
    "                dict_count[tag] = 1\n",
    "            else:\n",
    "                dict_count[tag]  += 1\n",
    "        all_sample_tags.append(dict_count)\n",
    "    \n",
    "    cc = pd.DataFrame.from_dict(all_sample_tags)\n",
    "    cc.fillna(0,inplace=True)\n",
    "    return cc\n",
    "\n",
    "def get_vector(model,all_data):\n",
    "    vector_sen = []\n",
    "    for d in all_data:\n",
    "        single_sen_vec = []\n",
    "        words = d.split(' ')\n",
    "        for w in words:\n",
    "            try:   \n",
    "                get_word_vec = model[w]\n",
    "            except:\n",
    "                pass\n",
    "            single_sen_vec.append(get_word_vec)\n",
    "        v = np.array(single_sen_vec).mean(axis=0)\n",
    "        vector_sen.append(v)\n",
    "    return vector_sen\n",
    "\n",
    "\n",
    "#Function so that one session can be called multiple times. \n",
    "#Useful while multiple calls need to be done for embedding. \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "def embed_useT(module):\n",
    "    with tf.Graph().as_default():\n",
    "        sentences = tf.placeholder(tf.string)\n",
    "        embed = hub.Module(module)\n",
    "        embeddings = embed(sentences)\n",
    "        session = tf.train.MonitoredSession()\n",
    "    return lambda x: session.run(embeddings, {sentences: x})\n",
    "\n",
    "embed_fn = embed_useT('/home/prashant/google_sen_vec/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_model(X_train, X_test, y_train_internal, y_test_internal,num_cla):\n",
    "    params = {\n",
    "                    'num_leaves': 70,\n",
    "                    'max_bin': 110,\n",
    "                    'num_class':num_cla,\n",
    "                    'min_data_in_leaf': 50,\n",
    "                    'learning_rate': 0.01,\n",
    "                    'min_sum_hessian_in_leaf': 0.000446,\n",
    "                    'bagging_fraction': 0.60,\n",
    "                    'bagging_freq': 15,\n",
    "                    'max_depth': 50,\n",
    "                    'save_binary': True,\n",
    "                    'seed': 31452,\n",
    "                    'feature_fraction_seed': 31415,\n",
    "                    'feature_fraction': 0.51,\n",
    "                    'bagging_seed': 31415,\n",
    "                    'drop_seed': 31415,\n",
    "                    'data_random_seed': 31415,\n",
    "                    'objective': 'multiclass',\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'verbose': 1,\n",
    "                    'metric': 'multi_logloss',\n",
    "                    'is_unbalance': False,\n",
    "\n",
    "            }\n",
    "    d_train = lgb.Dataset(X_train, label=y_train_internal)\n",
    "    clf = lgb.train(params, d_train,30000)\n",
    "    return clf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(single_df):\n",
    "    ner_tags = get_ner_tags(list(single_df['EssayText']))\n",
    "    #noun_p = get_noun_phrase(list(single_df['EssayText']))\n",
    "    \n",
    "    parts_dataset = get_part_of_speech_tags(list(single_df['EssayText']))\n",
    "\n",
    "    single_df['EssayText']=single_df['EssayText'].apply(preprocess_data)\n",
    "\n",
    "    sen_len = get_sen_len(list(single_df['EssayText']))\n",
    "\n",
    "    vectors = get_vector(model,list(single_df['EssayText']))\n",
    "    \n",
    "    \n",
    "    #spell_cnt = spelling_mistakes_check(list(single_df['EssayText']))\n",
    "    \n",
    "    senti = sentiment(list(single_df['EssayText']))\n",
    "\n",
    "    \n",
    "    new_tfidf_features = pd.DataFrame(vectors)\n",
    "    new_tfidf_features.columns = list(map(lambda x : str(x) , list(new_tfidf_features.columns)))\n",
    "    new_tfidf_features['clarity'] = list(single_df['clarity'])\n",
    "    #new_tfidf_features['spell_mis'] = spell_cnt\n",
    "    new_tfidf_features['sen_len'] = sen_len\n",
    "    new_tfidf_features['senti'] = senti\n",
    "    new_tfidf_features['coherent'] = list(single_df['coherent'])\n",
    "    new_tfidf_features['kitna_aacha'] = list(single_df['kitna_aacha'])\n",
    "    new_tfidf_features['source_'] = list(single_df['source_'])\n",
    "\n",
    "\n",
    "    new_tfidf_features = pd.concat([new_tfidf_features,parts_dataset,ner_tags],axis=1)\n",
    "    \n",
    "    new_tfidf_features.dropna(axis=0,inplace=True)\n",
    "    all_preprocessed_single_q_data =  pd.get_dummies(new_tfidf_features,columns=['clarity','coherent'])\n",
    "    preprocessed_main_test = all_preprocessed_single_q_data.loc[all_preprocessed_single_q_data['source_'] == 0]\n",
    "    preprocessed_main_train = all_preprocessed_single_q_data.loc[all_preprocessed_single_q_data['source_'] == 1]\n",
    "\n",
    "    preprocessed_main_test.drop(['source_','kitna_aacha'],axis =1,inplace=True)\n",
    "\n",
    "    y_train = (preprocessed_main_train['kitna_aacha'])\n",
    "    x_train = preprocessed_main_train.drop(['kitna_aacha','source_'],axis =1)\n",
    "    return x_train,y_train,preprocessed_main_test,parts_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_models(model,multi_datasets):\n",
    "    flag = 1\n",
    "    for i in multi_datasets:\n",
    "        print(i)\n",
    "        \n",
    "        single_df = multi_datasets[i]\n",
    "        single_df.reset_index(inplace= True)\n",
    "        single_df.dropna(axis=0,inplace=True)\n",
    "\n",
    "        test_id = list(single_df.loc[single_df['source_'] ==0]['ID'])\n",
    "        es = list(single_df.loc[single_df['source_'] ==0]['Essayset'])\n",
    "        if len(test_id) != len(es):\n",
    "            print('PANGA')\n",
    "            break\n",
    "\n",
    "        single_df['candi_score'] = single_df[['score_1','score_2' ,'score_3' ,'score_4' ,'score_5']].mean(axis=1)\n",
    "        single_df['candi_score'] = list(map(lambda x : round(x),single_df['candi_score']))\n",
    "        single_df.drop(['score_1','score_2' ,'score_3' ,'score_4' ,'score_5','ID','index'],inplace=True,axis=1)\n",
    "        single_df['kitna_aacha']=single_df['candi_score']\n",
    "        single_df.drop(['min_score','max_score','Essayset','candi_score'],inplace=True,axis=1)\n",
    "        \n",
    "        x_train,y_train,preprocessed_main_test,p = feature_eng(single_df)\n",
    "        X_train, X_test, y_train_internal, y_test_internal = train_test_split(x_train,y_train ,test_size =0.0)\n",
    "        from pprint import pprint\n",
    "        num_cla = len(set(y_train))\n",
    "        print('----')\n",
    "        print(num_cla) \n",
    "        clf = ml_model(X_train, X_test, y_train_internal, y_test_internal,num_cla)\n",
    "        #return X_train,clf\n",
    "        if flag ==0 :\n",
    "            pred_internal = clf.predict(X_test)\n",
    "            print(pred_internal)\n",
    "            pred_internal = list(map(lambda x : max(enumerate(x), key=operator.itemgetter(1))[0],pred_internal))\n",
    "            from sklearn import metrics\n",
    "            pp.append(metrics.accuracy_score(y_test_internal, pred_internal))\n",
    "        if flag ==1:\n",
    "            pred_internal = clf.predict(preprocessed_main_test)\n",
    "            pred_internal =list(map(lambda x : max(enumerate(x), key=operator.itemgetter(1))[0],pred_internal))\n",
    "            res =list(zip(test_id,es,pred_internal))\n",
    "            pp.append(res)\n",
    "    return pp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/miniconda2/envs/p3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('test_dataset.csv')\n",
    "df_test.head()\n",
    "df_train = pd.read_csv('train_dataset.csv')\n",
    "\n",
    "df_train['source_'] = 1\n",
    "df_test['source_'] =0\n",
    "\n",
    "df = pd.concat([df_train,df_test])\n",
    "df['score_1'].fillna(1, inplace = True)\n",
    "df['score_2'].fillna(1, inplace = True)\n",
    "df['score_3'].fillna(1, inplace = True)\n",
    "df['score_4'].fillna(1, inplace = True)\n",
    "df['score_5'].fillna(1, inplace = True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'],inplace=True,axis=1)\n",
    "\n",
    "multi_datasets = {}\n",
    "groups = df.groupby('Essayset')\n",
    "for name, group in groups:\n",
    "    multi_datasets[name] = group\n",
    "\n",
    "pp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prashant/miniconda2/envs/p3/lib/python3.6/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "4\n",
      "2.0\n",
      "----\n",
      "4\n",
      "3.0\n",
      "----\n",
      "3\n",
      "4.0\n",
      "----\n",
      "3\n",
      "5.0\n",
      "----\n",
      "4\n",
      "6.0\n",
      "----\n",
      "4\n",
      "7.0\n",
      "----\n",
      "3\n",
      "8.0\n",
      "----\n",
      "3\n",
      "9.0\n",
      "----\n",
      "3\n",
      "10.0\n",
      "----\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = different_models(model,multi_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(a.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# # sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "# feature_imp = pd.DataFrame(sorted(zip(b.feature_importance(),a.columns)), columns=['Value','Feature'])\n",
    "\n",
    "# plt.figure(figsize=(20, 100))\n",
    "# sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "# plt.title('LightGBM Features (avg over folds)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.savefig('lgbm_importances-01.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import itertools\n",
    "l = list(itertools.chain(*a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ = pd.DataFrame(l,columns=['id','essay_set','essay_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_.to_csv('/home/prashant/incedo/p2/incedo_participant/subsa_last/embed_pos_sen_len_with_para_tun_senti_ner_tags.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['I hate ','']\n",
    "sentiment(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
